{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nQJCUx74Vxo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import inflect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import wordnet\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "stem1=PorterStemmer()\n",
        "lemma=wordnet.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# String Functions"
      ],
      "metadata": {
        "id": "qp-Bj6zy8Aeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"This is a line\"\n",
        "b = \"Train\"\n",
        "print(a[3:8])\n",
        "print(a.lower())\n",
        "print(a.upper())\n",
        "print(a.replace(\"i\", \"o\"))\n",
        "print(a.split(\"i\"))\n",
        "print(a +\" \"+ b)"
      ],
      "metadata": {
        "id": "IjRKqdZJ66GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Functions"
      ],
      "metadata": {
        "id": "OOW9uTCe8u6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"data.txt\", \"w+\")\n",
        "for i in range(5):\n",
        "    file.write(\"Line number is %d\\r\\n\" % (i + 1))\n",
        "file.close()"
      ],
      "metadata": {
        "id": "e9YVjlxL8CFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file= open(\"data.txt\", \"a+\")\n",
        "for i in range(3):\n",
        "    file.write(\"Appending Line number %d\\r\\n\" % (i+1))\n",
        "file.close()"
      ],
      "metadata": {
        "id": "A0aggL2b8QjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file= open(\"datatest.txt\", \"r\")\n",
        "if file.mode == 'r':\n",
        "    content=file.read() #We used file.read() for reading the file data and store it in a variable.\n",
        "print(content)\n",
        "print(\"Here is the output!\")"
      ],
      "metadata": {
        "id": "l22k_i0-8lkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "S2lzrPHJ8zGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "input_str = \"This is Embarrasing\"\n",
        "lowercase_text(input_str)"
      ],
      "metadata": {
        "id": "qg4_SE_U8xZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_num(text):\n",
        "    result = re.sub(r'\\d+','', text)\n",
        "    return result\n",
        "input_s =\"You bought 6 shuttlle for 350 rs from the shop\"\n",
        "remove_num(input_s)"
      ],
      "metadata": {
        "id": "GMqAp0tx84w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rem_punct(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "input_str = \"Somebody PLease Help !!!, Is it you jack ?\"\n",
        "rem_punct(input_str)"
      ],
      "metadata": {
        "id": "zgGzUYlw89IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords Removal"
      ],
      "metadata": {
        "id": "aUh5Ep0b-tDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_text = [word for word in word_tokens if word not in string.punctuation]\n",
        "  filtered_text = [word for word in filtered_text if word not in stop_words]\n",
        "  return filtered_text\n",
        "\n",
        "text = \"Now lets begin the mega show. Open your eyes and heart to welcome most important guest of the night Mr.James Ken\"\n",
        "filtered_text = remove_stopwords(text)\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "id": "mq5Zc3Z293UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "YI55Usvj-r48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  stems = [stemmer.stem(word) for word in word_tokens]\n",
        "  return stems\n",
        "\n",
        "text = 'Now lets begin the mega show. Open your eyes and heart to welcome most important guest of the night Mr.James Ken.'\n",
        "stem_words(text)"
      ],
      "metadata": {
        "id": "WsheBMMx-Kdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "RNA-ocUE-plw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_word(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  lemmas = [wordnet.WordNetLemmatizer().lemmatize(word, pos='v') for word in word_tokens]\n",
        "  return lemmas\n",
        "\n",
        "text = \"Now lets begin the mega show. Open your eyes and heart to welcome most important guest of the night Mr.James Ken.\"\n",
        "lemmatized_words = lemmatize_word(text)\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "id": "AjRa7IdP-c3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging"
      ],
      "metadata": {
        "id": "nagQxki5-xiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagg(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return pos_tag(word_tokens)\n",
        "\n",
        "\n",
        "pos_tagg(\"What is your problem Fernandez ?\")"
      ],
      "metadata": {
        "id": "LjztvkFp-wQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking"
      ],
      "metadata": {
        "id": "Xuh5-9n__ApW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunking(text, grammar):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "  chunkParser = nltk.RegexpParser(grammar)\n",
        "  tree = chunkParser.parse(word_pos)\n",
        "  for subtree in tree.subtrees():\n",
        "    print(subtree)\n",
        "\n",
        "\n",
        "sentence = 'Roses are red, violets are blue. If you look down, you can see your shoe'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "chunking(sentence, grammar)"
      ],
      "metadata": {
        "id": "MCAmNlxs-8h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition"
      ],
      "metadata": {
        "id": "WshDF8MS_Uno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ner(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  word_pos = pos_tag(word_tokens)\n",
        "  print(ne_chunk(word_pos))\n",
        "\n",
        "text = 'England vs India was a fantastic match. It was breathtaking till the very end'\n",
        "ner(text)"
      ],
      "metadata": {
        "id": "jRUTtDXh_JIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex"
      ],
      "metadata": {
        "id": "AcSHKKcIA7lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "4eTUPxzqA_CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"dataset, Data is a new fuel\"\n",
        "r2 = re.findall(r\"^\\w+\", sent)\n",
        "\n",
        "print(r2)"
      ],
      "metadata": {
        "id": "RZfxgCuFA3Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((re.split(r'\\s', 'We splited this sentence')))"
      ],
      "metadata": {
        "id": "u7wwDfquA_9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lists = ['icecream images', 'i immitated', 'inner peace']\n",
        "\n",
        "for i in lists:\n",
        "    q = re.match(\"(i\\w+)\\W(i\\w+)\", i)\n",
        "\n",
        "    if q:\n",
        "        print(q.groups())"
      ],
      "metadata": {
        "id": "T6dUdOLjBGDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [\"playing\", \"dataset\"]\n",
        "text = \"Raju is playing outside.\"\n",
        "\n",
        "for p in pattern:\n",
        "    print(\"You're looking for '%s' in '%s'\" % (p, text), end='')\n",
        "\n",
        "    if re.search(p, text):\n",
        "        print('Found match!')\n",
        "    else:\n",
        "        print(\"no match found!\")"
      ],
      "metadata": {
        "id": "e3DryyHuBHqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}